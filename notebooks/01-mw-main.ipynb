{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "try:\n",
    "    import jupyter_black\n",
    "except:\n",
    "    print(\"Jupyter-Black not found\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied Machine Learning - LE3\n",
    "\n",
    "- Modell Selektion\n",
    "- Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from mlxtend import plotting\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions & Parameters\n",
    "\n",
    "Wir definieren einige Parameter wie Pfade und für die Visualisierung der Daten.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"../data\")\n",
    "\n",
    "HUE_ORDER=[\"Benign\", \"Malign\"]\n",
    "HUE_ORDER_NUM=[0, 1]\n",
    "\n",
    "\n",
    "colors= sns.color_palette().as_hex()\n",
    "COLORS =  [colors[0], colors[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun definieren wir einige Funktionen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_region(X, y, clf, ax, colors, title):\n",
    "    _ = plotting.plot_decision_regions(X=X, y=y.ravel(), clf=clf, ax=ax, scatter_kwargs={'s': 0}, colors=\",\".join(colors))\n",
    "    _ = sns.scatterplot(y=X[:, 1], x=X[:, 0], hue=y.ravel(), ax=ax, hue_order=HUE_ORDER_NUM, palette=colors).set(\n",
    "        title=title,\n",
    "        xlabel=\"Fläche Tumor\",\n",
    "        ylim=(0, X[:, 1].max()),\n",
    "        xlim=(X[:, 0].min(), X[:, 0].max()),\n",
    "        ylabel=\"Symmetrie\")\n",
    "    handles, _ = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles[2:], ['Benign', 'Malign'], framealpha=0.3, scatterpoints=1)\n",
    "\n",
    "\n",
    "def undersample_malign(df, n: int = 80):\n",
    "    # Undersample the Malign class\n",
    "    df_malign = (\n",
    "        df\n",
    "        .query(\"diagnosis == 'Malign'\")\n",
    "        .sample(n=n, random_state=123)\n",
    "    )\n",
    "\n",
    "    # Get all Benign samples\n",
    "    df_benign = df.query(\"diagnosis == 'Benign'\")\n",
    "\n",
    "    # Combine to create a balanced dataset\n",
    "    return pd.concat([df_benign, df_malign])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Wir lesen nun einen Datensatz ein. Dieser Datensatz ist aus der Medizin und beinhaltet Messwerte von Tumoren in der Brust mit Verdacht auf Brustkrebs. Diese Tumore wurden entsprechend diagnostiziert und in gutartig (benign) und bösartig (malign) eingestuft. Der Datensatz hat verschiedene Attribute, wobei wir uns auf einige wenige beschränken werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv(DATA_PATH.joinpath(\"breast-cancer.csv\")).drop('Unnamed: 32', axis=1)\n",
    "\n",
    "df = (\n",
    "    df_raw\n",
    "    .assign(diagnosis=lambda _df: _df['diagnosis'].map({'M': \"Malign\", 'B': \"Benign\"}))\n",
    "    .astype({\"diagnosis\": \"category\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir schauen uns den Datensatz mal grob an."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"diagnosis\", observed=True).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für Experiment-Zwecke können wir die Klassen-Imbalance noch vergrössern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_breast_cancer = undersample_malign(df_breast_cancer, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir wählen zwei Features aus. Dadurch kann man den Datensatz gut darstellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"diagnosis\", \"symmetry_worst\", \"area_mean\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "_ = sns.scatterplot(data=df, y=\"symmetry_worst\", x='area_mean', hue='diagnosis', ax=ax, hue_order=HUE_ORDER, palette=COLORS).set(\n",
    "    title=\"Diagnose in Abhängigkeit der Fläche und Symmetrie\",\n",
    "    xlabel=\"Fläche Tumor\",\n",
    "    ylabel=\"Symmetrie\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lässt sich das Problem gut modellieren?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun bereiten wir die Daten für die Modellierung vor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create data matrix X and label vector y\n",
    "X2d = df[['area_mean', 'symmetry_worst']].to_numpy()\n",
    "X = df.drop([\"id\", \"diagnosis\"], axis=1).to_numpy()\n",
    "y = df[['diagnosis']].to_numpy().reshape(-1,)\n",
    "\n",
    "label_encoder = LabelEncoder().fit(y)\n",
    "y = label_encoder.transform(y)\n",
    "\n",
    "print(label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Erste Modelle\n",
    "\n",
    "Wir trainieren nun erste Modelle und schauen uns die Ergebnisse an."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(123)\n",
    "\n",
    "# train a logistic regression model\n",
    "clf = LogisticRegression()\n",
    "clf = clf.fit(X2d, y)\n",
    "\n",
    "# train a random forest classifier\n",
    "clf2 = RandomForestClassifier(n_estimators=50, random_state=rng)\n",
    "clf2 = clf2.fit(X=X2d, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(16, 7), ncols=2)\n",
    "plot_decision_region(X2d, y, clf, axes[0], COLORS, title=\"Logistic Regression\")\n",
    "plot_decision_region(X2d, y, clf2, axes[1], COLORS, title=\"Random Forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wie gefallen Euch die Modelle?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy Logistic Regression: {clf.score(X2d, y):.2f}\")\n",
    "print(f\"Accuracy Random Forest: {clf2.score(X2d, y):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Offensichtlich: Random Forest is the Winner!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modell Selektion\n",
    "\n",
    "So geht's natürlich nicht. Wir brauchen ein unabhängiges Set um die Modelle fair zu evaluieren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Train Test Split](../figures/train_test.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir generieren nun Splits für Training, Validation und Test.\n",
    "\n",
    "\n",
    "Dazu verwenden wir: [sklearn.model_selection.train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
    "\n",
    "\n",
    "Wir _stratifizieren_ dabei nach der Zielvariablen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X2d_train, X2d_test, y_train, y_test = train_test_split(\n",
    "    X2d, y, test_size=0.2, random_state=123, stratify=y)\n",
    "\n",
    "X2d_train, X2d_val, y_train, y_val = train_test_split(\n",
    "    X2d_train, y_train, test_size=0.2, random_state=123, stratify=y_train)\n",
    "\n",
    "\n",
    "clf = LogisticRegression(random_state=123)\n",
    "clf = clf.fit(X2d_train, y_train)\n",
    "\n",
    "# train a random forest classifier\n",
    "rng = np.random.RandomState(123)\n",
    "clf2 = RandomForestClassifier(n_estimators=50, random_state=rng)\n",
    "clf2 = clf2.fit(X=X2d_train, y=y_train)\n",
    "\n",
    "\n",
    "print(f\"Accuracy Logistic Regression: {clf.score(X2d_val, y_val):.2f}\")\n",
    "print(f\"Accuracy Random Forest: {clf2.score(X2d_val, y_val):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun können wir das beste Modell auf dem Testset evaluieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy Random Forest: {clf2.score(X2d_test, y_test):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie robust ist dieses Resultat?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltas = list()\n",
    "for random_state in range(0, 100):\n",
    "\n",
    "    X2d_train, X2d_test, y_train, y_test = train_test_split(\n",
    "        X2d, y, test_size=0.2, random_state=123, stratify=y)\n",
    "    X2d_train, X2d_val, y_train, y_val = train_test_split(\n",
    "        X2d_train, y_train, test_size=0.2, random_state=random_state, stratify=y_train)\n",
    "\n",
    "    clf = LogisticRegression(random_state=123)\n",
    "    clf = clf.fit(X2d_train, y_train)\n",
    "\n",
    "    # train a random forest classifier\n",
    "    rng = np.random.RandomState(123)\n",
    "    clf2 = RandomForestClassifier(n_estimators=50, random_state=rng)\n",
    "    clf2 = clf2.fit(X=X2d_train, y=y_train)\n",
    "\n",
    "    deltas.append(clf.score(X2d_val, y_val) - clf2.score(X2d_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "_ = sns.histplot(deltas, bins=np.arange(-0.06, 0.06, 0.01), ax=ax).set(\n",
    "    title=\"Differenz Accuracy Logistic Regression - Random Forest\", xlabel=\"Differenz\", ylabel=\"Häufigkeit\")\n",
    "\n",
    "print(f\"Anzahl Cases bei der die Logistische Regression besser wäre: {sum(np.array(deltas) > 0) / len(deltas):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kreuzvalidierung\n",
    "\n",
    "Ein robusteres Verfahren ist die Kreuzvalidierung."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Kreuzvalidierung](../figures/xval.jpg)\n",
    "\n",
    "Quelle: _Sebastian Raschka and Vahid Mirjalili. Python Machine Learning, 3rd Ed. Packt Publishing, Birmingham, UK, 3 edition, 2019. ISBN 978-1-78995-575-0._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ein Beispiel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "X2d_train, X2d_test, y_train, y_test = train_test_split(\n",
    "    X2d, y, test_size=0.2, random_state=123, stratify=y)\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "\n",
    "results  = list()\n",
    "\n",
    "for train_idx, validation_idx in cv.split(X2d_train, y_train):\n",
    "\n",
    "    X_train_split, y_train_split = X2d_train[train_idx], y_train[train_idx]\n",
    "    X_val_split, y_val_split = X2d_train[validation_idx], y_train[validation_idx]\n",
    "\n",
    "    clf = LogisticRegression()\n",
    "    clf = clf.fit(X_train_split, y_train_split)\n",
    "\n",
    "    rng = np.random.RandomState(123)\n",
    "    clf2 = RandomForestClassifier(n_estimators=50, random_state=rng)\n",
    "    clf2 = clf2.fit(X=X_train_split, y=y_train_split)\n",
    "\n",
    "    results.append({\"model\": \"Logistic Regression\", \"accuracy\": clf.score(X_val_split, y_val_split)})\n",
    "    results.append({\"model\": \"Random Forest\", \"accuracy\": clf2.score(X_val_split, y_val_split)})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame.from_records(results)\n",
    "df_results.groupby(\"model\").agg({\"accuracy\": [\"mean\", \"std\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "_ = sns.barplot(df_results, x=\"model\", y=\"accuracy\", errorbar=\"sd\", ax=ax).set(\n",
    "    title=\"Performance Vergleich Kreuzvalidierung\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Was passiert wenn man mehr oder weniger Folds wählt?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-Parameter Optimierung\n",
    "\n",
    "\n",
    "Die meisten Algorithmen haben Hyper-Parameter welche den Optimisierungsprozess beeinflussen. Diese können wir nicht direkt mit dem Trainingsset optimieren, sondern müssen ein Validation Set verwenden.\n",
    "\n",
    "\n",
    "Wir möchten z.B. testen ob mehr Bäume (`n_estimators`) besser sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2d_train, X2d_test, y_train, y_test = train_test_split(\n",
    "    X2d, y, test_size=0.2, random_state=123, stratify=y)\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "\n",
    "results  = list()\n",
    "\n",
    "rng = np.random.RandomState(123)\n",
    "\n",
    "for train_idx, validation_idx in cv.split(X2d_train, y_train):\n",
    "\n",
    "    X_train_split, y_train_split = X2d_train[train_idx], y_train[train_idx]\n",
    "    X_val_split, y_val_split = X2d_train[validation_idx], y_train[validation_idx]\n",
    "\n",
    "    clf = LogisticRegression(random_state=123)\n",
    "    clf = clf.fit(X_train_split, y_train_split)\n",
    "\n",
    "    \n",
    "    clf2 = RandomForestClassifier(n_estimators=50, random_state=rng)\n",
    "    clf2 = clf2.fit(X=X_train_split, y=y_train_split)\n",
    "\n",
    "    clf3 = RandomForestClassifier(n_estimators=150, random_state=rng)\n",
    "    clf3 = clf3.fit(X=X_train_split, y=y_train_split)\n",
    "\n",
    "    results.append({\"model\": \"Logistic Regression\", \"accuracy\": clf.score(X_val_split, y_val_split)})\n",
    "    results.append({\"model\": \"Random Forest\", \"accuracy\": clf2.score(X_val_split, y_val_split)})\n",
    "    results.append({\"model\": \"Random Forest2\", \"accuracy\": clf3.score(X_val_split, y_val_split)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame.from_records(results)\n",
    "df_results.groupby(\"model\").agg({\"accuracy\": [\"mean\", \"std\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame.from_records(results)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "_ = sns.barplot(df_results, x=\"model\", y=\"accuracy\", errorbar=\"sd\", ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ist ein Random Forest mit mehr Bäumen besser?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid-Search\n",
    "\n",
    "Man kann verschiedene Hyper-Parameter Kombinationen einfach mit [sklearn.model_selection.GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) ausprobieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "\n",
    "rng = np.random.RandomState(123)\n",
    "\n",
    "algorithms = {\n",
    "    \"random_forest\": {\n",
    "        \"cls\": RandomForestClassifier(random_state=rng),\n",
    "        \"hyper_params\": {\n",
    "            \"n_estimators\": [50, 100, 200],\n",
    "            \"max_depth\": [1, 3, 5, 10, 15]\n",
    "        }\n",
    "    },\n",
    "    \"logistic_regression\": {\n",
    "        \"cls\": LogisticRegression(random_state=123),\n",
    "        \"hyper_params\": {\n",
    "            \"C\": [0.1,  0.0001]\n",
    "        }\n",
    "    },\n",
    "    \"mlp\": {\n",
    "        \"cls\": MLPClassifier(random_state=rng),\n",
    "        \"hyper_params\": {\n",
    "            \"hidden_layer_sizes\": [[5, 5], [10, 10]],\n",
    "            \"max_iter\": [1000]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "for algorithm_name, algorithm_data in algorithms.items():\n",
    "\n",
    "    param_grid = algorithm_data[\"hyper_params\"]\n",
    "    algorithm = algorithm_data[\"cls\"]\n",
    "    grid_search = GridSearchCV(\n",
    "            algorithm,\n",
    "            param_grid=param_grid,\n",
    "            cv=cv,\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "    \n",
    "    grid_search.fit(X2d_train, y_train)\n",
    "\n",
    "    # Best parameters and estimator\n",
    "    print(\"Best parameters:\", grid_search.best_params_)\n",
    "    print(\"Best estimator:\", grid_search.best_estimator_)\n",
    "    print(f\"Best score: {grid_search.best_score_:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warum ist das MLP so schlecht?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 6), ncols=1)\n",
    "plot_decision_region(X2d_train, y_train, grid_search.best_estimator_, ax, COLORS, title=\"MLP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ist das eine erwartete Decision Bundary?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Was könnte man verbessern?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines\n",
    "\n",
    "Pipelines vereinfachen die Hyper-Parameter Optimisierung und Integrieren Pre-Processing Schritte in ein gemeinsames Objekt, welches man Fitten und auf neuen Daten anwenden kann. Es verringert die Gefahr von Fehlern und Data Leakage enorm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Pipelines](../figures/ml_pipelines.jpg)\n",
    "\n",
    "Quelle: _Sebastian Raschka and Vahid Mirjalili. Python Machine Learning, 3rd Ed. Packt Publishing, Birmingham, UK, 3 edition, 2019. ISBN 978-1-78995-575-0._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "\n",
    "rng = np.random.RandomState(123)\n",
    "\n",
    "algorithms = {\n",
    "    \"random_forest\": {\n",
    "        \"cls\": RandomForestClassifier(random_state=rng),\n",
    "        \"hyper_params\": {\n",
    "            \"cls__n_estimators\": [50, 100, 200],\n",
    "            \"cls__max_depth\": [1, 3, 5, 10, 15],\n",
    "        }\n",
    "    },\n",
    "    \"logistic_regression\": {\n",
    "        \"cls\": LogisticRegression(random_state=rng),\n",
    "        \"hyper_params\": {\n",
    "            \"cls__C\": [0.1,  0.0001],\n",
    "        }\n",
    "    },\n",
    "    \"mlp\": {\n",
    "    \"cls\": MLPClassifier(random_state=rng),\n",
    "    \"hyper_params\": {\n",
    "        \"cls__hidden_layer_sizes\": [[5, 5], [10, 10]],\n",
    "        \"cls__max_iter\": [1000],\n",
    "    }\n",
    "}\n",
    "}\n",
    "\n",
    "for algorithm_name, algorithm_data in algorithms.items():\n",
    "\n",
    "    param_grid = algorithm_data[\"hyper_params\"]\n",
    "    algorithm = algorithm_data[\"cls\"]\n",
    "\n",
    "    pipeline = Pipeline([(\"scaler\", StandardScaler()), (\"cls\", algorithm)])\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "            pipeline,\n",
    "            param_grid=param_grid,\n",
    "            cv=cv,\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "    \n",
    "    grid_search.fit(X2d_train, y_train)\n",
    "\n",
    "    # Best parameters and estimator\n",
    "    print(\"Best parameters:\", grid_search.best_params_)\n",
    "    print(\"Best estimator:\", grid_search.best_estimator_)\n",
    "    print(f\"Best score: {grid_search.best_score_:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie sieht die Decision Boundary aus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 6), ncols=1)\n",
    "plot_decision_region(X2d_train, y_train, grid_search.best_estimator_, ax, COLORS, title=\"MLP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Was wenn ich verschiedene Pre-Processing Schritte ausprobieren möchte?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "rng = np.random.RandomState(123)\n",
    "\n",
    "# Combined pipeline with conditional preprocessing\n",
    "pipeline = Pipeline([\n",
    "    ('pre_processing', 'passthrough'),  # placeholder\n",
    "    ('cls', LogisticRegression())       # placeholder\n",
    "])\n",
    "\n",
    "param_grid = [\n",
    "    # Random Forest (no scaling)\n",
    "    {\n",
    "        \"pre_processing\": [\"passthrough\"],\n",
    "        \"cls\": [RandomForestClassifier(random_state=rng)],\n",
    "        \"cls__n_estimators\": [50, 100, 200],\n",
    "        \"cls__max_depth\": [1, 3, 5, 10, 15],\n",
    "    },\n",
    "    # Logistic Regression (with scaling)\n",
    "    {\n",
    "        \"pre_processing\": [StandardScaler(), MinMaxScaler()],\n",
    "        \"cls\": [LogisticRegression(random_state=rng)],\n",
    "        \"cls__C\": [0.1, 0.0001],\n",
    "    },\n",
    "    {\n",
    "        \"pre_processing\": [StandardScaler(), MinMaxScaler()],\n",
    "        \"cls\": [MLPClassifier(random_state=rng)],\n",
    "        \"cls__hidden_layer_sizes\": [[5, 5], [10, 10]],\n",
    "        \"cls__max_iter\": [1000],\n",
    "    }\n",
    "]\n",
    "\n",
    "# GridSearch with conditional preprocessing\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "    \n",
    "grid_search.fit(X2d_train, y_train)\n",
    "\n",
    "# Best parameters and estimator\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best estimator:\", grid_search.best_estimator_)\n",
    "print(f\"Best score: {grid_search.best_score_:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame.from_records(grid_search.cv_results_)\n",
    "df_results.sort_values(\"rank_test_score\").head(5)[[\"mean_test_score\", \"param_cls\", \"param_cls__C\", \"param_cls__n_estimators\", \"param_cls__max_depth\", \"param_pre_processing\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 6), ncols=1)\n",
    "plot_decision_region(X2d_train, y_train, grid_search.best_estimator_, ax, COLORS, title=\"Best Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metriken\n",
    "\n",
    "Wie messe ich die Güte / Performance von einem Modell?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "_ = sns.scatterplot(data=df, y=\"symmetry_worst\", x='area_mean', hue='diagnosis', ax=ax, hue_order=HUE_ORDER, palette=COLORS).set(\n",
    "    title=\"Diagnose in Abhängigkeit der Fläche und Symmetrie\",\n",
    "    xlabel=\"Fläche Tumor\",\n",
    "    ylabel=\"Symmetrie\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "# Obtain the best estimator pipeline from GridSearchCV\n",
    "best_pipeline = grid_search.best_estimator_\n",
    "\n",
    "# Use cross_val_predict to get validation predictions across folds\n",
    "y_pred_cv = cross_val_predict(best_pipeline, X2d_train, y_train, cv=cv, n_jobs=-1)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_train, y_pred_cv)\n",
    "\n",
    "# Display confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "_ = disp.plot(cmap='Blues')\n",
    "_ = plt.title('Confusion Matrix (Cross-validated predictions)')\n",
    "plt.show()\n",
    "\n",
    "num_false_negatives = cm[1][0] / cm[1].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{100 * num_false_negatives:.2f}% der positiven Fälle werden nicht erkannt.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "rng = np.random.RandomState(123)\n",
    "\n",
    "# Combined pipeline with conditional preprocessing\n",
    "pipeline = Pipeline([\n",
    "    ('pre_processing', 'passthrough'),  # placeholder\n",
    "    ('cls', LogisticRegression())       # placeholder\n",
    "])\n",
    "\n",
    "# Updated param grid to conditionally apply scaling\n",
    "param_grid = [\n",
    "    # Random Forest (no scaling)\n",
    "    {\n",
    "        \"pre_processing\": [\"passthrough\"],\n",
    "        \"cls\": [RandomForestClassifier(random_state=rng)],\n",
    "        \"cls__n_estimators\": [50, 100, 200],\n",
    "        \"cls__max_depth\": [1, 3, 5, 10, 15],\n",
    "    },\n",
    "    # Logistic Regression (with scaling)\n",
    "    {\n",
    "        \"pre_processing\": [StandardScaler()],\n",
    "        \"cls\": [LogisticRegression(random_state=rng)],\n",
    "        \"cls__C\": [0.1, 0.0001],\n",
    "    },\n",
    "    {\n",
    "        \"pre_processing\": [StandardScaler()],\n",
    "        \"cls\": [MLPClassifier(random_state=rng)],\n",
    "        \"cls__hidden_layer_sizes\": [[5, 5], [10, 10]],\n",
    "        \"cls__max_iter\": [1000],\n",
    "    }\n",
    "]\n",
    "\n",
    "# GridSearch with conditional preprocessing\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=cv,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "grid_search.fit(X2d_train, y_train)\n",
    "\n",
    "# Best parameters and estimator\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best estimator:\", grid_search.best_estimator_)\n",
    "print(f\"Best score: {grid_search.best_score_:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pipeline = grid_search.best_estimator_\n",
    "\n",
    "# Use cross_val_predict to get validation predictions across folds\n",
    "y_pred_cv = cross_val_predict(best_pipeline, X2d_train, y_train, cv=cv, n_jobs=-1)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_train, y_pred_cv)\n",
    "\n",
    "# Display confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title('Confusion Matrix (Cross-validated predictions)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Was, wenn wir eine komplexere Kostenfuntion haben?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, make_scorer\n",
    "\n",
    "\n",
    "def cost_metric(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    \n",
    "    # Define your costs here (example values):\n",
    "    cost_fp = 5  # Cost of false positive\n",
    "    cost_fn = 50  # Cost of false negative\n",
    "    cost_tp = 0   # Benefit (negative cost) of true positive\n",
    "    cost_tn = 0   # Cost of true negative (typically zero)\n",
    "    \n",
    "    # Compute total cost\n",
    "    total_cost = (fp * cost_fp) + (fn * cost_fn) + (tp * cost_tp) + (tn * cost_tn)\n",
    "    return total_cost\n",
    "\n",
    "# Create scorer (lower is better, hence greater_is_better=False)\n",
    "cost_scorer = make_scorer(cost_metric, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir können diese verwenden für die Grid-Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "rng = np.random.RandomState(123)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('pre_processing', 'passthrough'),  # placeholder\n",
    "    ('cls', LogisticRegression())       # placeholder\n",
    "])\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        \"pre_processing\": [\"passthrough\"],\n",
    "        \"cls\": [RandomForestClassifier(random_state=rng)],\n",
    "        \"cls__n_estimators\": [50, 100, 200],\n",
    "        \"cls__max_depth\": [1, 3, 5, 10, 15],\n",
    "    },\n",
    "    {\n",
    "        \"pre_processing\": [StandardScaler()],\n",
    "        \"cls\": [LogisticRegression(random_state=123)],\n",
    "        \"cls__C\": [0.1, 0.0001],\n",
    "    },\n",
    "    {\n",
    "        \"pre_processing\": [StandardScaler()],\n",
    "        \"cls\": [MLPClassifier(random_state=rng)],\n",
    "        \"cls__hidden_layer_sizes\": [[5, 5], [10, 10]],\n",
    "        \"cls__max_iter\": [1000],\n",
    "    }\n",
    "]\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=cv,\n",
    "    scoring=cost_scorer,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "grid_search.fit(X2d_train, y_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best estimator:\", grid_search.best_estimator_)\n",
    "print(f\"Best score: {grid_search.best_score_:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pipeline = grid_search.best_estimator_\n",
    "\n",
    "# Use cross_val_predict to get validation predictions across folds\n",
    "y_pred_cv = cross_val_predict(best_pipeline, X2d_train, y_train, cv=cv, n_jobs=-1)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_train, y_pred_cv)\n",
    "\n",
    "# Display confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title('Confusion Matrix (Cross-validated predictions)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Kurve\n",
    "\n",
    "Wichtiges Instrument um einen Classifier zu beurteilen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=1, figsize=(14, 6))\n",
    "\n",
    "y_pred_cv = cross_val_predict(best_pipeline, X2d_train, y_train, cv=cv, n_jobs=-1, method=\"predict_proba\")\n",
    "\n",
    "RocCurveDisplay.from_predictions(\n",
    "    y_train, y_pred_cv[:, 1].ravel(), pos_label=1, ax=axs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning the Threshold\n",
    "\n",
    "Bei Klassifikations-Problemen möchte man oft Precision vs Recall optimieren. \n",
    "\n",
    "Dies kann man tun, indem man den Decision-Threshold optimiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TunedThresholdClassifierCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_model = TunedThresholdClassifierCV(\n",
    "    estimator=grid_search.best_estimator_,\n",
    "    cv=cv,\n",
    "    scoring=cost_scorer,\n",
    "    store_cv_results=True,  # necessary to inspect all results\n",
    ")\n",
    "\n",
    "tuned_model.fit(X2d_train, y_train)\n",
    "print(f\"{tuned_model.best_threshold_=:0.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use cross_val_predict to get validation predictions across folds\n",
    "y_pred_cv = cross_val_predict(tuned_model, X2d_train, y_train, cv=cv, n_jobs=-1)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_train, y_pred_cv)\n",
    "\n",
    "# Display confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title('Confusion Matrix (Cross-validated predictions)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Achtung positiver Bias!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=1, figsize=(14, 6))\n",
    "\n",
    "y_pred_cv = cross_val_predict(tuned_model, X2d_train, y_train, cv=cv, n_jobs=-1, method=\"predict_proba\")\n",
    "\n",
    "RocCurveDisplay.from_predictions(\n",
    "    y_train, y_pred_cv[:, 1].ravel(), pos_label=1, ax=axs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversampling\n",
    "\n",
    "Wir haben ja gesehen, dass die positive Klasse unterrepräsentiert ist. Könnte Oversampling helfen?\n",
    "\n",
    "Es gibt dazu ein eigenes sklearn-kompatibles Package.\n",
    "\n",
    "[imbalanced-learn](https://imbalanced-learn.org/stable/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import ADASYN, SMOTE\n",
    "\n",
    "X2d_resampled_train, y_resampled_train = SMOTE(random_state=123).fit_resample(X2d_train, y_train)\n",
    "\n",
    "print(f\"Sampled: {X2d_resampled_train.shape[0] - X2d_train.shape[0]}\")\n",
    "\n",
    "np.bincount(y_resampled_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "rng = np.random.RandomState(123)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('pre_processing', 'passthrough'),  # placeholder\n",
    "    ('cls', LogisticRegression())       # placeholder\n",
    "])\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        \"pre_processing\": [\"passthrough\"],\n",
    "        \"cls\": [RandomForestClassifier(random_state=rng)],\n",
    "        \"cls__n_estimators\": [50, 100, 200],\n",
    "        \"cls__max_depth\": [1, 3, 5, 10, 15],\n",
    "    },\n",
    "    {\n",
    "        \"pre_processing\": [StandardScaler()],\n",
    "        \"cls\": [LogisticRegression(random_state=123)],\n",
    "        \"cls__C\": [0.1, 0.0001],\n",
    "    },\n",
    "    {\n",
    "        \"pre_processing\": [StandardScaler()],\n",
    "        \"cls\": [MLPClassifier(random_state=rng)],\n",
    "        \"cls__hidden_layer_sizes\": [[5, 5], [10, 10]],\n",
    "        \"cls__max_iter\": [1000],\n",
    "    }\n",
    "]\n",
    "\n",
    "# GridSearch with conditional preprocessing\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=cv,\n",
    "    scoring=\"roc_auc\",\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "grid_search.fit(X2d_resampled_train, y_resampled_train)\n",
    "\n",
    "# Best parameters and estimator\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best estimator:\", grid_search.best_estimator_)\n",
    "print(f\"Best score: {grid_search.best_score_:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=1, figsize=(14, 6))\n",
    "\n",
    "y_pred_cv = cross_val_predict(grid_search.best_estimator_, X2d_resampled_train, y_resampled_train, cv=cv, n_jobs=-1, method=\"predict_proba\")\n",
    "\n",
    "RocCurveDisplay.from_predictions(\n",
    "    y_resampled_train, y_pred_cv[:, 1].ravel(), pos_label=1, ax=axs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Alles gut?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "rng = np.random.RandomState(123)\n",
    "\n",
    "# Combined pipeline with conditional preprocessing\n",
    "pipeline = ImbPipeline([\n",
    "    ('pre_processing', 'passthrough'),  \n",
    "    ('sampling', 'passthrough'),        \n",
    "    ('cls', LogisticRegression())       # placeholder\n",
    "])\n",
    "\n",
    "# Updated param grid to conditionally apply scaling\n",
    "param_grid = [\n",
    "    # Random Forest (no scaling)\n",
    "    {\n",
    "        \"pre_processing\": [\"passthrough\"],\n",
    "        \"cls\": [RandomForestClassifier(random_state=rng)],\n",
    "        \"cls__n_estimators\": [50, 100, 200],\n",
    "        \"cls__max_depth\": [1, 3, 5, 10, 15],\n",
    "        \"sampling\": [\"passthrough\"],\n",
    "    },\n",
    "    {\n",
    "        \"pre_processing\": [\"passthrough\"],\n",
    "        \"cls\": [RandomForestClassifier(random_state=rng)],\n",
    "        \"cls__n_estimators\": [50, 100, 200],\n",
    "        \"cls__max_depth\": [1, 3, 5, 10, 15],\n",
    "        \"sampling\": [SMOTE(random_state=rng)],\n",
    "        'sampling__k_neighbors': [3, 5, 7],\n",
    "    },\n",
    "    {\n",
    "        \"pre_processing\": [StandardScaler()],\n",
    "        \"cls\": [LogisticRegression(random_state=rng)],\n",
    "        \"cls__class_weight\": [\"balanced\", None, {1: 10, 0: 1}],\n",
    "        \"cls__C\": [0.1, 0.0001],\n",
    "        \"sampling\": [\"passthrough\"],\n",
    "    },\n",
    "    {\n",
    "        \"pre_processing\": [StandardScaler()],\n",
    "        \"cls\": [LogisticRegression(random_state=rng)],\n",
    "        \"cls__class_weight\": [\"balanced\", None, {1: 10, 0: 1}],\n",
    "        \"cls__C\": [0.1, 0.0001],\n",
    "        \"sampling\": [SMOTE(random_state=rng)],\n",
    "        'sampling__k_neighbors': [3, 5, 7],\n",
    "    },\n",
    "    {\n",
    "        \"pre_processing\": [StandardScaler()],\n",
    "        \"cls\": [MLPClassifier(random_state=rng)],\n",
    "        \"cls__hidden_layer_sizes\": [[5, 5], [10, 10]],\n",
    "        \"cls__max_iter\": [1000],\n",
    "        \"sampling\": [\"passthrough\"],\n",
    "    },\n",
    "    {\n",
    "        \"pre_processing\": [StandardScaler()],\n",
    "        \"cls\": [MLPClassifier(random_state=rng)],\n",
    "        \"cls__hidden_layer_sizes\": [[5, 5], [10, 10]],\n",
    "        \"cls__max_iter\": [1000],\n",
    "        \"sampling\": [SMOTE(random_state=rng)],\n",
    "        'sampling__k_neighbors': [3, 5, 7],\n",
    "    }\n",
    "]\n",
    "\n",
    "# GridSearch with conditional preprocessing\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=cv,\n",
    "    scoring=\"roc_auc\",\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "\n",
    "grid_search.fit(X2d_train, y=y_train)\n",
    "\n",
    "# Best parameters and estimator\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best estimator:\", grid_search.best_estimator_)\n",
    "print(f\"Best score: {grid_search.best_score_:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame.from_records(grid_search.cv_results_)\n",
    "df_results.sort_values(\"rank_test_score\").head(5)[[\"mean_test_score\", \"param_cls\", \"param_cls__C\", \"param_cls__n_estimators\", \"param_cls__max_depth\", \"param_pre_processing\", \"param_sampling\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=1, figsize=(14, 6))\n",
    "\n",
    "y_pred_cv = cross_val_predict(grid_search.best_estimator_, X2d_train, y_train, cv=cv, n_jobs=-1, method=\"predict_proba\")\n",
    "\n",
    "RocCurveDisplay.from_predictions(\n",
    "    y_train, y_pred_cv[:, 1].ravel(), pos_label=1, ax=axs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noch etwas eleganter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "rng = np.random.RandomState(123)\n",
    "\n",
    "# Combined pipeline with conditional preprocessing\n",
    "pipeline = ImbPipeline([\n",
    "    ('pre_processing', 'passthrough'),  \n",
    "    ('sampling', 'passthrough'),        \n",
    "    ('cls', LogisticRegression())       # placeholder\n",
    "])\n",
    "\n",
    "\n",
    "param_grid_algos = [\n",
    "    {\n",
    "        \"pre_processing\": [\"passthrough\"],\n",
    "        \"cls\": [RandomForestClassifier(random_state=rng)],\n",
    "        \"cls__n_estimators\": [200],\n",
    "        \"cls__max_depth\": [1, 3, 5, 10, None],\n",
    "    },\n",
    "    {\n",
    "        \"pre_processing\": [StandardScaler()],\n",
    "        \"cls\": [LogisticRegression(random_state=rng)],\n",
    "        \"cls__class_weight\": [\"balanced\", None, {1: 10, 0: 1}],\n",
    "        \"cls__C\": [0.15, 0.1, 0.01, 0.001],\n",
    "    },\n",
    "    {\n",
    "        \"pre_processing\": [StandardScaler()],\n",
    "        \"cls\": [MLPClassifier(random_state=rng)],\n",
    "        \"cls__hidden_layer_sizes\": [[5, 5], [10, 10]],\n",
    "        \"cls__max_iter\": [1000],\n",
    "    }\n",
    "]\n",
    "\n",
    "param_grid_sampling = [\n",
    "    {\n",
    "        'sampling': [SMOTE(random_state=rng)],\n",
    "        'sampling__k_neighbors': [3, 5, 7],\n",
    "    },\n",
    "    {\n",
    "        'sampling': ['passthrough'],\n",
    "    }\n",
    "]\n",
    "\n",
    "param_grid = []\n",
    "for algo_grid in param_grid_algos:\n",
    "    for sampling_grid in param_grid_sampling:\n",
    "        combined_grid = {**algo_grid, **sampling_grid}\n",
    "        param_grid.append(combined_grid)\n",
    "\n",
    "\n",
    "# GridSearch with conditional preprocessing\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=cv,\n",
    "    scoring=\"roc_auc\",\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "\n",
    "grid_search.fit(X2d_train, y=y_train)\n",
    "\n",
    "# Best parameters and estimator\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best estimator:\", grid_search.best_estimator_)\n",
    "print(f\"Best score: {grid_search.best_score_:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame.from_records(grid_search.cv_results_)\n",
    "df_results.sort_values(\"rank_test_score\").head(5)[[\"mean_test_score\", \"param_cls\", \"param_cls__C\", \"param_cls__n_estimators\", \"param_cls__max_depth\", \"param_pre_processing\", \"param_sampling\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir verwenden Stratified KFold um die Modelle zu evaluieren.  \n",
    "\n",
    "Auch möchten wir PCA als Pre-Processing Schritt verwenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n",
    "rng = np.random.RandomState(123)\n",
    "\n",
    "# Combined pipeline with conditional preprocessing\n",
    "pipeline = ImbPipeline([\n",
    "    ('sampling', 'passthrough'),\n",
    "    ('pre_processing', StandardScaler()),\n",
    "    ('pca', 'passthrough'),  # conditional PCA step\n",
    "    ('cls', LogisticRegression())\n",
    "])\n",
    "\n",
    "\n",
    "param_grid_algos = [\n",
    "    {\n",
    "        \"pre_processing\": [\"passthrough\"],\n",
    "        \"pca\": [\"passthrough\"],\n",
    "        \"cls\": [RandomForestClassifier(random_state=rng)],\n",
    "        \"cls__n_estimators\": [200],\n",
    "        \"cls__max_depth\": [1, 3, 5, 10, None],\n",
    "        \"sampling\": [SMOTE(k_neighbors=k) for k in [3, 5, 7]] + ['passthrough'],\n",
    "    },\n",
    "    {\n",
    "        \"pre_processing\": [StandardScaler()],\n",
    "        \"pca\": [\"passthrough\"],\n",
    "        \"cls\": [LogisticRegression(random_state=rng)],\n",
    "        \"cls__class_weight\": [\"balanced\", None, {1: 10, 0: 1}],\n",
    "        \"cls__C\": [0.15, 0.1, 0.01, 0.001],\n",
    "        \"sampling\": [SMOTE(k_neighbors=k) for k in [3, 5, 7]] + ['passthrough'],\n",
    "    },\n",
    "    {\n",
    "        \"pre_processing\": [StandardScaler()],\n",
    "        \"pca\": [PCA()],\n",
    "        \"pca__n_components\": [2, 3, 4, 5, 6],\n",
    "        \"cls\": [LogisticRegression(random_state=rng)],\n",
    "        \"cls__class_weight\": [\"balanced\", None, {1: 10, 0: 1}],\n",
    "        \"cls__C\": [0.15, 0.1, 0.01, 0.001],\n",
    "        \"sampling\": [SMOTE(k_neighbors=k) for k in [3, 5, 7]] + ['passthrough'],\n",
    "    },\n",
    "    {\n",
    "        \"pre_processing\": [StandardScaler()],\n",
    "        \"pca\": [PCA(n_components=n) for n in [3, 5, 7]] + ['passthrough'],\n",
    "        \"cls\": [MLPClassifier(random_state=rng)],\n",
    "        \"cls__hidden_layer_sizes\": [[5, 5], [10, 10], [20, 20]],\n",
    "        \"cls__max_iter\": [1000],\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "# GridSearch with conditional preprocessing\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=cv,\n",
    "    scoring=\"roc_auc\",\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y=y_train)\n",
    "\n",
    "# Best parameters and estimator\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best estimator:\", grid_search.best_estimator_)\n",
    "print(f\"Best score: {grid_search.best_score_:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_model = TunedThresholdClassifierCV(\n",
    "    estimator=grid_search.best_estimator_,\n",
    "    cv=cv,\n",
    "    scoring=cost_scorer,\n",
    "    store_cv_results=True,  # necessary to inspect all results\n",
    ")\n",
    "\n",
    "tuned_model.fit(X_train, y_train)\n",
    "print(f\"{tuned_model.best_threshold_=:0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Performance Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test = tuned_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_hat_test )\n",
    "\n",
    "# Display confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title('Confusion Matrix (Cross-validated predictions)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "cost_metric(y_test, y_hat_test)\n",
    "print(classification_report(y_test, y_hat_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
